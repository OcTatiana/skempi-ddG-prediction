# -*- coding: utf-8 -*-
"""biocad ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12sd3BKaVlTBmrvA-u8wpn88uGgNcq2aX
"""

#! pip install Bio


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


R = 1.987e-3

data = pd.read_csv("skempi_v2.csv", sep=";")

# В Skempi нет явно посчитанного значения ddG, однако его можно вычислить из аффинности мутанта и дикого типа
def calculate_ddg(row):
    Kd_mut = 1 / row['Affinity_mut_parsed']  # предполагаем, что колонка 7 - аффинность мутанта
    Kd_wt = 1 / row['Affinity_wt_parsed']    # колонка 8 - аффинность дикого типа
    ddG = R * row['Temperature_parsed'] * np.log(Kd_mut / Kd_wt)
    return ddG


data["Temperature_parsed"] = data["Temperature"].str.replace("(assumed)", "").astype(float)
data['ddG'] = data.apply(calculate_ddg, axis=1)
data['ddG_sign'] = (data['ddG'] > 0).astype(int)  # 1 если ddG > 0, иначе 0


# Разделение строк с несколькими вариантами замен на отдельные
data = data.assign(**{"Mutation(s)_PDB": data["Mutation(s)_PDB"].str.split(',')})
data = data.assign(**{"Mutation(s)_cleaned": data["Mutation(s)_cleaned"].str.split(',')})
data = data.assign(**{"iMutation_Location(s)": data["iMutation_Location(s)"].str.split(',')})
data = data.explode(["Mutation(s)_PDB", "Mutation(s)_cleaned", "iMutation_Location(s)"]).reset_index(drop=True)

data['Wild_AA'] = data['Mutation(s)_PDB'].str[0]  # исходная аминокислота
data['Mutated_AA'] = data['Mutation(s)_PDB'].str[-1]  # мутированная аминокислота
le = LabelEncoder()
data['Wild_AA_encoded'] = le.fit_transform(data['Wild_AA'])
data['Mutated_AA_encoded'] = le.fit_transform(data['Mutated_AA'])


# Используя BLOSUM62 вычислим консервативность замены, а также будем учитывать изменение в гидрофобности
blosum62 = matrix = substitution_matrices.load("BLOSUM62")
aa_num = "ARNDCQEGHILKMFPSTWYVBZX*"

aa_hydrophobicity = {
    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,
    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,
    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,
    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2
}

data["Hydrophobicity_change"] = data["Mutated_AA"].map(aa_hydrophobicity) - data["Wild_AA"].map(aa_hydrophobicity)
data["BLOSUM62"] = data.apply(lambda row: blosum62.get((aa_num.index(row["Wild_AA"]), aa_num.index(row["Mutated_AA"])), 0), axis=1)


def mutation_type(wild, mutated):
    if wild == mutated:
        return "neutral"
    elif (wild in "DEHKR" and mutated in "DEHKR") or (wild in "FILMV" and mutated in "FILMV"):
        return "conservative"
    else:
        return "radical"


data["Mutation_type"] = data.apply(lambda row: mutation_type(row["Wild_AA"], row["Mutated_AA"]), axis=1)
data = pd.get_dummies(data, columns=["Mutation_type"])
data = pd.get_dummies(data, columns=["iMutation_Location(s)"])


# Для обучения модели будем использовать аминокислотные замены, их характеристики и
# положение относительно сайта связывания

features = ["Wild_AA_encoded",
            "Mutated_AA_encoded",
            "Hydrophobicity_change",
            "BLOSUM62",
            "Mutation_type_conservative",
             "Temperature_parsed",
            "iMutation_Location(s)_COR",
            "iMutation_Location(s)_INT",
            "iMutation_Location(s)_RIM",
            "iMutation_Location(s)_SUP",
            "Hold_out_type",
            "ddG_sign"
            ]

filtered_data = data[features]
filtered_data = filtered_data.dropna()

# Ручное разбиение для cross-validation по типам комплексов
train_data = filtered_data.query("Hold_out_type == 'AB/AG'")
test_data = filtered_data.query("Hold_out_type != 'AB/AG'")

X_train = train_data[features[:-2]]
X_test = test_data[features[:-2]]
y_train = train_data["ddG_sign"]
y_test = test_data["ddG_sign"]


### Логистическая регрессия
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"F1-score: {f1_score(y_test, y_pred):.2f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred):.2f}")

# Accuracy: 0.76
# F1-score: 0.06
# ROC-AUC: 0.49


### Метод случайного леса
model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"F1-score: {f1_score(y_test, y_pred):.2f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred):.2f}")

# Accuracy: 0.68
# F1-score: 0.16
# ROC-AUC: 0.48


### Метод опорных векторов
model = SVC(kernel='rbf')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"F1-score: {f1_score(y_test, y_pred):.2f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred):.2f}")

# Accuracy: 0.81
# F1-score: 0.00
# ROC-AUC: 0.50

# Все оценки просто ужасны, и ничего не предсказывают.
# Вероятно, сказывается дисбаланс классов, либо выбраны не те признаки для предсказания